{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KPIs and Status Summary\n",
    "This notebook calculates a number of KPI metrics and a summary of status for test results given the user-specified query and grouping parameters. It ties into the **Test Monitor Service** for retrieving filtered test results, the **Notebook Execution Service** for running outside of Jupyterhub, and **Grafana Dashboards** for displaying results.\n",
    "\n",
    "The parameters and output use a schema recognized by the NI Plotly Graph Plugin for Grafana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "Import Python modules for executing the notebook. Pandas is used for building and handling dataframes. Scrapbook is used for recording data for the Notebook Execution Service. The SystemLink Test Monitor Client provides access to test result data for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scrapbook as sb\n",
    "from dateutil import tz\n",
    "\n",
    "import systemlink.clients.nitestmonitor as testmon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "- `results_filter`: Dynamic Linq query filter for test results from the Test Monitor Service\n",
    "  - Options: Any valid Test Monitor Results Dynamic Linq filter\n",
    "  - Default: `'startedWithin <= \"30.0:0:0\"'`\n",
    "- `products_filter`: Dynamic Linq query filter for test results based on products from the Test Monitor Service\n",
    "  - Options: Any valid Test Monitor Products Dynamic Linq filter\n",
    "  - Default: `''`\n",
    "- `group_by`: The dimension along which to reduce; what each bar in the output graph represents  \n",
    "  - Options: Day, System, Test Program, Operator, Part Number  \n",
    "  - Default: `'Day'`\n",
    "\n",
    "Parameters are also listed in the metadata for the parameters cell, along with their default values. The Notebook Execution services uses that metadata to pass parameters from the Test Monitor Reports page to this notebook. Available `group_by` options are listed in the metadata as well; the Test Monitor Reports page uses these to validate inputs sent to the notebook.\n",
    "\n",
    "To see the metadata, select the code cell and click the wrench icon in the far left panel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "parameters": {
      "group_by": "Day",
      "products_filter": "",
      "results_filter": "startedWithin <= \"30.0:0:0\""
     }
    },
    "systemlink": {
     "namespaces": [
      "ni-testmanagement"
     ],
     "outputs": [
      {
       "display_name": "KPIs",
       "id": "kpis_graph",
       "type": "data_frame"
      },
      {
       "display_name": "Status Counts",
       "id": "status_count_graph",
       "type": "data_frame"
      }
     ],
     "parameters": [
      {
       "display_name": "Group By",
       "id": "group_by",
       "options": [
        "Day",
        "System",
        "Test Program",
        "Operator",
        "Part Number",
        "Workspace"
       ],
       "type": "string"
      },
      {
       "default_display": {
        "products_filter": [],
        "results_filter": [
         {
          "queryOperandUnit": "DAYS",
          "queryOperandValue": 30,
          "queryOperator": "LESS_THAN_OR_EQUAL",
          "queryOperatorName": "startedWithin"
         }
        ]
       },
       "display_name": "Query by",
       "id": "results_filter",
       "type": "test_monitor_result_query"
      }
     ],
     "version": 2
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "results_filter = 'partNumber == \"HR-3UTG-AMZN\" && startedWithin <= \"30.0:0:0\"'\n",
    "products_filter = ''\n",
    "group_by = 'Day'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping from grouping options to Test Monitor terminology\n",
    "Translate the grouping options shown in the Test Monitor Reports page to keywords recognized by the Test Monitor API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_map = {\n",
    "    'Day': 'started_at',\n",
    "    'System': 'system_id',\n",
    "    'Test Program': 'program_name',\n",
    "    'Operator': 'operator',\n",
    "    'Part Number': 'part_number',\n",
    "    'Workspace': 'workspace'\n",
    "}\n",
    "grouping = groups_map[group_by]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Test Monitor client\n",
    "Establish a connection to SystemLink over HTTP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_api = testmon.ResultsApi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query for results\n",
    "Query the Test Monitor Service for results matching the `results_filter` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_query = testmon.ResultsAdvancedQuery(\n",
    "    results_filter, product_filter=products_filter, order_by=testmon.ResultField.STARTED_AT)\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "response = await results_api.query_results_v2(post_body=results_query)\n",
    "while response.continuation_token:\n",
    "    results = results + response.results\n",
    "    results_query.continuation_token = response.continuation_token\n",
    "    response = await results_api.query_results_v2(post_body=results_query)\n",
    "\n",
    "results_list = [result.to_dict() for result in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get group names\n",
    "Collect the group name for each result based on the `group_by` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_names = []\n",
    "for result in results_list:\n",
    "    if grouping in result:\n",
    "        group_names.append(result[grouping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pandas dataframe\n",
    "Put the data into a dataframe whose columns are serial number, status, start time, and group name. Sort and group the dataframe to get the first test run for each unique serial number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_results = {\n",
    "    'id': [result['id'] for result in results_list],\n",
    "    'serial_number': [result['serial_number'] for result in results_list],\n",
    "    'status': [result['status']['status_type'] for result in results_list],\n",
    "    'started_at': [result['started_at'] for result in results_list],\n",
    "    'system_id': [result['system_id'] for result in results_list],\n",
    "    'total_time_in_seconds': [result['total_time_in_seconds'] for result in results_list],\n",
    "    grouping: group_names\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame.from_dict(formatted_results)\n",
    "\n",
    "if grouping == 'started_at':\n",
    "    sorting_list = ['serial_number', 'started_at']\n",
    "    grouping_list = ['serial_number']\n",
    "\n",
    "else:\n",
    "    sorting_list = [grouping, 'serial_number', 'started_at']\n",
    "    grouping_list = [grouping, 'serial_number']\n",
    "\n",
    "df_results = df_results.sort_values(by=sorting_list)\n",
    "df_results_unique = df_results.groupby(grouping_list).first().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle grouping by day\n",
    "If the grouping is by day, the group name is the date and time when the test started in UTC. To group all test results from a single day together, convert to server time and remove time information from the group name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_copy = copy.copy(df_results_unique)\n",
    "df_results_copy.fillna(value='', inplace=True)\n",
    "\n",
    "if grouping == 'started_at':\n",
    "    truncated_times = []\n",
    "    for val in df_results_copy[grouping]:\n",
    "        local_time = val.astimezone(tz.tzlocal())\n",
    "        truncated_times.append(str(datetime.date(local_time.year, local_time.month, local_time.day)))\n",
    "    df_results_copy[grouping] = truncated_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate results into groups\n",
    "Aggregate the data for each unique group and status.\n",
    "\n",
    "*See documentation for [size](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.size.html) and [unstack](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.unstack.html) here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped_status = df_results_copy.groupby([grouping, 'status']).size().unstack(fill_value=0)\n",
    "if 'PASSED' not in df_grouped_status:\n",
    "    df_grouped_status['PASSED'] = 0\n",
    "if 'FAILED' not in df_grouped_status:\n",
    "    df_grouped_status['FAILED'] = 0\n",
    "if 'ERRORED' not in df_grouped_status:\n",
    "    df_grouped_status['ERRORED'] = 0\n",
    "if 'TERMINATED' not in df_grouped_status:\n",
    "    df_grouped_status['TERMINATED'] = 0\n",
    "    \n",
    "df_grouped_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KPI calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Pass Yield - Divide the number of passed tests by the total number of tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first_pass_yield = pd.DataFrame(100 * df_grouped_status['PASSED'] / (df_grouped_status['FAILED'] + df_grouped_status['ERRORED'] + df_grouped_status['PASSED']))\n",
    "\n",
    "df_kpis = df_first_pass_yield.reset_index().set_axis([grouping, 'yield'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughput - Count the number of tests per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_throughput = df_results_copy.groupby(grouping).agg({'id': 'count'})\n",
    "df_throughput = df_throughput.reset_index().set_axis([grouping, 'throughput'], axis=1)\n",
    "\n",
    "df_kpis = df_kpis.merge(df_throughput, on=grouping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Failure Rate - Divide the number of failed and errored tests by the total number of tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fail_rate = pd.DataFrame(100 * (df_grouped_status['FAILED'] + df_grouped_status['ERRORED'])/ (df_grouped_status['FAILED'] + df_grouped_status['ERRORED'] + df_grouped_status['PASSED']))\n",
    "\n",
    "if df_fail_rate.empty:\n",
    "    df_fail_rate = pd.DataFrame(columns=[grouping, 'fail_rate'])\n",
    "    df_kpis['fail_rate'] = ''\n",
    "else:\n",
    "    df_fail_rate = df_fail_rate.reset_index().set_axis([grouping, 'fail_rate'], axis=1)\n",
    "    df_kpis = df_kpis.merge(df_fail_rate, on=grouping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "System Utilization - Divide how much time testing occurred by the time available for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_availability_by_day = {\n",
    "    'MONDAY': 8,\n",
    "    'TUESDAY': 8,\n",
    "    'WEDNESDAY': 8,\n",
    "    'THURSDAY': 8,\n",
    "    'FRIDAY': 8,\n",
    "    'SATURDAY': 0,\n",
    "    'SUNDAY': 0\n",
    "}\n",
    "\n",
    "number_of_unique_systems = df_results['system_id'].nunique()\n",
    "\n",
    "dates = []\n",
    "for val in df_results['started_at']:\n",
    "    dates.append(val)\n",
    "if dates:\n",
    "    dates.sort()\n",
    "    start_date = datetime.datetime(dates[0].year, dates[0].month, dates[0].day)\n",
    "    end_date = datetime.datetime(dates[-1].year, dates[-1].month, dates[-1].day) + datetime.timedelta(days=1)\n",
    "\n",
    "group_info = {\n",
    "    grouping: [],\n",
    "    'utilization': []\n",
    "}\n",
    "\n",
    "for group in df_results_copy[grouping].unique():\n",
    "    group_df = df_results_copy[df_results_copy[grouping] == group]\n",
    "    group_info[grouping].append(group)\n",
    "    current_date = start_date\n",
    "    available_in_seconds = 0\n",
    "    while current_date < end_date:\n",
    "        available_in_seconds += system_availability_by_day[current_date.strftime('%A').upper()] * 60 * 60\n",
    "        current_date += datetime.timedelta(days=1)\n",
    "    group_info['utilization'].append(100 * (group_df['total_time_in_seconds'].sum() / available_in_seconds))\n",
    "\n",
    "df_system_utilization = pd.DataFrame.from_dict(group_info)\n",
    "\n",
    "if grouping == 'started_at':\n",
    "    df_system_utilization['utilization'] = df_system_utilization['utilization'] / number_of_unique_systems\n",
    "else:\n",
    "    df_system_utilization.sort_values(by=['utilization'], ascending=True, inplace=True)\n",
    "    \n",
    "df_kpis = df_kpis.merge(df_system_utilization, on=grouping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grouping == 'started_at':\n",
    "    df_kpis['started_at'] = pd.to_datetime(df_kpis['started_at'])\n",
    "else:\n",
    "    df_kpis.sort_values(by=['yield'], ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the dataframe to the SystemLink reports output format\n",
    "The result format for a SystemLink report consists of a list of output objects as defined below:\n",
    "- `type`: The type of the output. Accepted values are 'data_frame' and 'scalar'.\n",
    "- `id`: Corresponds to the id specified in the 'output' metadata. Used for returning multiple outputs with the 'V2' report format.\n",
    "- `data`: A dict representing the 'data_frame' type output data.\n",
    "    - `columns`: A list of dicts containing the names and data type for each column in the dataframe.\n",
    "    - `values`: A list of lists containing the dataframe values. The sublists are ordered according to the 'columns' configuration.\n",
    "- `value`: The value returned for the 'scalar' output type.\n",
    "- `config`: The configurations for the given output.\n",
    "    - `title`: The output title.\n",
    "    - `graph`: The graph configurations.\n",
    "        - `axis_labels`: The x-axis label and y-axis label.\n",
    "        - `plots`: A list of plots to display mapped from the dataframe's columns, along with configuration options.\n",
    "            - `x`: The dataframe column corresponding to the x-axis values.\n",
    "            - `y`: The dataframe column corresponding to the y-axis values.\n",
    "            - `style`: The plot's style. Accepted values are ['LINE', 'BAR', 'SCATTER'].\n",
    "            - `color`: The plot's color. Accepted formats are ['blue', '#0000ff', 'rbg(0,0,255)'].\n",
    "            - `label`: The plot's name, to be shown in a plot legend. \n",
    "            - `secondary_y`: Whether or not to display this plot on a second y-axis.\n",
    "            - `group_by`: A list of columns in the dataframe on which to group data, e.g. to color individual points.\n",
    "        - `orientation`: 'HORIZONTAL' or 'VERTICAL'.\n",
    "        - `stacked`: Whether or not to display the plots stacked on top of each other.\n",
    "\n",
    "For this notebook, there are two outputs.\n",
    "The first is a dataframe with five columns representing KPIs. For a grouping of 'Day', the first column contains ISO-8601 date strings. For any other grouping option, the first column contains categorical string values. The second column contains numerical values representing the yield percentages, etc.\n",
    "\n",
    "| started_at                 | yield         | throughput | fail_rate         | utilization|\n",
    "|----------------------------|---------------|------------|-------------------|------------|\n",
    "| '2020-09-29'               | 84.87         | 456        | 15.13             | 30.25      | \n",
    "| '2020-09-30'               | 65.58         | 154        | 37.85             | 10.5       |\n",
    "| '2020-10-01'               | 86.43         | 258        | 8.53              | 25.75      |\n",
    "\n",
    "The second is a dataframe with five columns representing status summary data.  For a grouping of 'Day', the first column contains ISO-8601 date strings. For any other grouping option, the first column contains categorical string values. The second column contains the count of passed tests, etc.\n",
    "\n",
    "| started_at                 | PASSED        | FAILED     | ERRORED           | TERMINATED |\n",
    "|----------------------------|---------------|------------|-------------------|------------|\n",
    "| '2020-09-29'               | 387           | 54         | 15                | 0          | \n",
    "| '2020-09-30'               | 101           | 27         | 26                | 0          |\n",
    "| '2020-10-01'               | 245           | 15         | 7                 | 13         |\n",
    "\n",
    "\n",
    "The notebook does not specify any configuration or plot information since Grafana users will provide this information in their dashboard configuration instead. We use Pandas to convert the dataframe built in the previous cells into a tabular format and then return that with the result object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict_kpis = {\n",
    "    'columns': pd.io.json.build_table_schema(df_kpis, index=False)['fields'],\n",
    "    'values': df_kpis.values.tolist(),\n",
    "}\n",
    "\n",
    "kpis_graph = {\n",
    "    'type': 'data_frame',\n",
    "    'id': 'kpis_graph',\n",
    "    'data': df_dict_kpis\n",
    "}\n",
    "\n",
    "df_grouped_status.reset_index(inplace=True)\n",
    "df_dict_status_count = {\n",
    "    'columns': pd.io.json.build_table_schema(df_grouped_status, index=False)['fields'],\n",
    "    'values': df_grouped_status.values.tolist(),\n",
    "}\n",
    "\n",
    "status_count_graph = {\n",
    "    'type': 'data_frame',\n",
    "    'id': 'status_count_graph',\n",
    "    'data': df_dict_status_count\n",
    "}\n",
    "\n",
    "result = [kpis_graph, status_count_graph]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record results with Scrapbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.glue('result', result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
